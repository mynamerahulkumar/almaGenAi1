{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bb25e9",
   "metadata": {},
   "source": [
    "# ü¶úüîó LangChain Fundamentals with Groq API\n",
    "\n",
    "Welcome to **LangChain Fundamentals**! This notebook will teach you the core concepts of LangChain using the **free Groq API**.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Setup & Configuration** - Environment setup with Groq API\n",
    "2. **ChatGroq Model** - Initialize and use Groq's LLM\n",
    "3. **Messages** - SystemMessage, HumanMessage, AIMessage\n",
    "4. **Prompt Templates** - Reusable prompts with variables\n",
    "5. **LCEL Chains** - Combine components with the `|` operator\n",
    "6. **Output Parsers** - Parse and structure LLM responses\n",
    "7. **Streaming & Batching** - Efficient response handling\n",
    "8. **Tools** - Function calling with LangChain\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Get your free Groq API key at: https://console.groq.com/keys\n",
    "- Add it to your `.env` file as `GROQ_API_KEY=your_key_here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd144ec",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's install and import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd892dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# %pip install langchain langchain-groq python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# If .env doesn't work, you can set the API key directly:\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-groq-api-key-here\"\n",
    "\n",
    "# Verify API key is loaded\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è API Key not found! Please either:\")\n",
    "    print(\"   1. Create a .env file with: GROQ_API_KEY=your-key-here\")\n",
    "    print(\"   2. Or uncomment and set the os.environ line above\")\n",
    "else:\n",
    "    print(f\"‚úÖ API Key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82ea63",
   "metadata": {},
   "source": [
    "## 2. Initialize ChatGroq Model\n",
    "\n",
    "**ChatGroq** is LangChain's integration with Groq's fast inference API. \n",
    "\n",
    "### Available Free Groq Models:\n",
    "| Model | Speed | Best For |\n",
    "|-------|-------|----------|\n",
    "| `llama-3.3-70b-versatile` | 280 tok/s | General purpose, high quality |\n",
    "| `llama-3.1-8b-instant` | 560 tok/s | Fast responses, simpler tasks |\n",
    "| `mixtral-8x7b-32768` | 450 tok/s | Complex reasoning |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e013e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3b4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eebeadb",
   "metadata": {},
   "source": [
    "## 3. Messages in LangChain\n",
    "\n",
    "LangChain uses **message types** to structure conversations:\n",
    "\n",
    "- **SystemMessage**: Sets the behavior/role of the AI\n",
    "- **HumanMessage**: User's input\n",
    "- **AIMessage**: AI's response (for conversation history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afb6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77acb5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2d16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5744147",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates\n",
    "\n",
    "**Prompt Templates** let you create reusable prompts with variables. This is essential for building scalable LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed03bfbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c41279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a124a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79f08d4",
   "metadata": {},
   "source": [
    "## 5. LCEL Chains (LangChain Expression Language)\n",
    "\n",
    "**LCEL** is LangChain's way to compose components using the **pipe operator `|`**. It's like Unix pipes for LLM applications!\n",
    "\n",
    "```\n",
    "prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This creates a chain where data flows: `input ‚Üí prompt ‚Üí llm ‚Üí output_parser ‚Üí output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff07da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b29fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f6ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d531c5ef",
   "metadata": {},
   "source": [
    "## 6. Output Parsers - Structured Output\n",
    "\n",
    "Output parsers transform LLM text responses into structured data like **JSON** or **Pydantic models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1a158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c1087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35ef48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a0f3d8",
   "metadata": {},
   "source": [
    "## 7. Streaming & Batching\n",
    "\n",
    "- **Streaming**: Get responses token-by-token (great for UX)\n",
    "- **Batching**: Process multiple inputs in parallel (great for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fb521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d426964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a242a6",
   "metadata": {},
   "source": [
    "## 8. Tools & Function Calling\n",
    "\n",
    "**Tools** allow the LLM to call external functions. This is the foundation for building **AI agents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ab396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca0777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a91d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd93e69",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "In this notebook, you learned the **LangChain Fundamentals**:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **ChatGroq** | Initialize Groq models with temperature, max_tokens |\n",
    "| **Messages** | SystemMessage, HumanMessage, AIMessage for conversations |\n",
    "| **Prompt Templates** | Create reusable prompts with variables |\n",
    "| **LCEL Chains** | Compose components with the `\\|` operator |\n",
    "| **Output Parsers** | Get structured data using Pydantic models |\n",
    "| **Streaming** | Token-by-token response for better UX |\n",
    "| **Batching** | Process multiple inputs efficiently |\n",
    "| **Tools** | Let LLMs call external functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps: RAG Fundamentals\n",
    "In the next notebook, we'll cover **Retrieval Augmented Generation (RAG)**:\n",
    "- Document loaders\n",
    "- Text splitters  \n",
    "- Embeddings\n",
    "- Vector stores\n",
    "- Retrieval chains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-basic-test-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
