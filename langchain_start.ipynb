{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bb25e9",
   "metadata": {},
   "source": [
    "# ðŸ¦œðŸ”— LangChain Fundamentals with Groq API\n",
    "\n",
    "Welcome to **LangChain Fundamentals**! This notebook will teach you the core concepts of LangChain using the **free Groq API**.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Setup & Configuration** - Environment setup with Groq API\n",
    "2. **ChatGroq Model** - Initialize and use Groq's LLM\n",
    "3. **Messages** - SystemMessage, HumanMessage, AIMessage\n",
    "4. **Prompt Templates** - Reusable prompts with variables\n",
    "5. **LCEL Chains** - Combine components with the `|` operator\n",
    "6. **Output Parsers** - Parse and structure LLM responses\n",
    "7. **Streaming & Batching** - Efficient response handling\n",
    "8. **Tools** - Function calling with LangChain\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Get your free Groq API key at: https://console.groq.com/keys\n",
    "- Add it to your `.env` file as `GROQ_API_KEY=your_key_here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd144ec",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's install and import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd892dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# %pip install langchain langchain-groq python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4463ed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# If .env doesn't work, you can set the API key directly:\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-groq-api-key-here\"\n",
    "\n",
    "# Verify API key is loaded\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"âš ï¸ API Key not found! Please either:\")\n",
    "    print(\"   1. Create a .env file with: GROQ_API_KEY=your-key-here\")\n",
    "    print(\"   2. Or uncomment and set the os.environ line above\")\n",
    "else:\n",
    "    print(f\"âœ… API Key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82ea63",
   "metadata": {},
   "source": [
    "## 2. Initialize ChatGroq Model\n",
    "\n",
    "**ChatGroq** is LangChain's integration with Groq's fast inference API. \n",
    "\n",
    "### Available Free Groq Models:\n",
    "| Model | Speed | Best For |\n",
    "|-------|-------|----------|\n",
    "| `llama-3.3-70b-versatile` | 280 tok/s | General purpose, high quality |\n",
    "| `llama-3.1-8b-instant` | 560 tok/s | Fast responses, simpler tasks |\n",
    "| `mixtral-8x7b-32768` | 450 tok/s | Complex reasoning |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e013e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model init {'llama-3.3-70b-versatile'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm =ChatGroq(\n",
    "    model='llama-3.3-70b-versatile',\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    max_retries=2,\n",
    "    \n",
    ")\n",
    "print(f\" Model init\",{llm.model_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb3b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm.invoke(\"What is langchain?,write in one sentence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219426bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework that enables developers to build applications powered by large language models, providing a set of tools and libraries to interact with and fine-tune these models.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebeadb",
   "metadata": {},
   "source": [
    "## 3. Messages in LangChain\n",
    "\n",
    "LangChain uses **message types** to structure conversations:\n",
    "\n",
    "- **SystemMessage**: Sets the behavior/role of the AI\n",
    "- **HumanMessage**: User's input\n",
    "- **AIMessage**: AI's response (for conversation history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14afb6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Lists vs Tuples**\n",
      "\n",
      "In Python, `lists` and `tuples` are two types of data structures that can store multiple values. The main difference between them is:\n",
      "\n",
      "* **Immutability**: Tuples are **immutable**, meaning you cannot change their values after creation. Lists are **mutable**, meaning you can change their values after creation.\n",
      "* **Syntax**: Tuples use parentheses `()` and lists use square brackets `[]`.\n",
      "\n",
      "**Example:**\n",
      "```python\n",
      "# Tuple (immutable)\n",
      "my_tuple = (1, 2, 3)\n",
      "print(my_tuple)  # Output: (1, 2, 3)\n",
      "# my_tuple[0] = 10  # This will raise an error\n",
      "\n",
      "# List (mutable)\n",
      "my_list = [1, 2, 3]\n",
      "print(my_list)  # Output: [1, 2, 3]\n",
      "my_list[0] = 10\n",
      "print(my_list)  # Output: [10, 2, 3]\n",
      "```\n",
      "When to use each:\n",
      "\n",
      "* Use **tuples** when you need to store a collection of values that shouldn't be changed, like a record or a constant.\n",
      "* Use **lists** when you need to store a collection of values that may need to be changed, like a dynamic array or a queue.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful python tutor,Keep explanation simple and use examples\"),\n",
    "    HumanMessage(content=\"What is difference between list and tuple?\")\n",
    "]\n",
    "response=llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77acb5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Rahul.\n"
     ]
    }
   ],
   "source": [
    "conversation=[\n",
    "    SystemMessage(content=\"You are a friendly AI assistant\"),\n",
    "    HumanMessage(content=\"My name is Rahul\"),\n",
    "    AIMessage(content=\"Nice to meet you,Rahul! How can I hep you?\"),\n",
    "    HumanMessage(content=\"What is my Name?\")\n",
    "]\n",
    "response=llm.invoke(conversation)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2d16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5744147",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates\n",
    "\n",
    "**Prompt Templates** let you create reusable prompts with variables. This is essential for building scalable LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee2c651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted messages\n",
      "system: You are expert in Gen AI.Provide clear and consice answer\n",
      "human: What is one Major advantage of Transformer model?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are expert in {topic}.Provide clear and consice answer\"),\n",
    "    (\"human\",\"{question}\"),\n",
    "])\n",
    "format_prompt=prompt.invoke({\n",
    "    \"topic\":\"Gen AI\",\n",
    "    \"question\":\"What is one Major advantage of Transformer model?\"\n",
    "}\n",
    ")\n",
    "print(\"Formatted messages\")\n",
    "for msg in format_prompt.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03bfbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95c41279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Parallelization**: One major advantage of the Transformer model is its ability to be parallelized, allowing for much faster training times compared to traditional recurrent neural networks (RNNs).\n"
     ]
    }
   ],
   "source": [
    "response=llm.invoke(format_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a124a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted messages\n",
      "system: You are expert in Health science.Provide clear and consice answer\n",
      "human: What is advantage of running \n"
     ]
    }
   ],
   "source": [
    "format_prompt=prompt.invoke({\n",
    "    \"topic\":\"Health science\",\n",
    "    \"question\":\"What is advantage of running \"\n",
    "}\n",
    ")\n",
    "print(\"Formatted messages\")\n",
    "for msg in format_prompt.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e16f81a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The advantages of running include:\n",
      "\n",
      "1. **Improves Cardiovascular Health**: Running strengthens the heart and lungs, improving circulation and reducing the risk of heart disease.\n",
      "2. **Weight Management**: Running helps burn calories and maintain a healthy weight.\n",
      "3. **Increases Mental Well-being**: Running releases endorphins, also known as \"feel-good\" hormones, which can help reduce stress and anxiety.\n",
      "4. **Boosts Immune System**: Running can help stimulate the immune system, reducing the risk of illness and infection.\n",
      "5. **Improves Sleep**: Regular running can help improve sleep quality and duration.\n",
      "6. **Increases Bone Density**: Running can help strengthen bones, reducing the risk of osteoporosis and fractures.\n",
      "7. **Reduces Risk of Chronic Diseases**: Running can help reduce the risk of chronic diseases, such as diabetes, certain types of cancer, and stroke.\n",
      "8. **Improves Mental Clarity and Focus**: Running can help improve cognitive function and mental clarity.\n",
      "9. **Increases Energy Levels**: Running can help increase energy levels and reduce fatigue.\n",
      "10. **Enhances Overall Physical Fitness**: Running can help improve overall physical fitness, including endurance, speed, and agility.\n"
     ]
    }
   ],
   "source": [
    "response=llm.invoke(format_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fc5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79f08d4",
   "metadata": {},
   "source": [
    "## 5. LCEL Chains (LangChain Expression Language)\n",
    "\n",
    "**LCEL** is LangChain's way to compose components using the **pipe operator `|`**. It's like Unix pipes for LLM applications!\n",
    "\n",
    "```\n",
    "prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This creates a chain where data flows: `input â†’ prompt â†’ llm â†’ output_parser â†’ output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks are like a computer brain that learns from data. They're made of layers of \"neurons\" that process information, similar to how our brain works. \n",
      "\n",
      "1. Input layer: Receives data (e.g., images, text)\n",
      "2. Hidden layers: Process and analyze the data\n",
      "3. Output layer: Produces a result (e.g., prediction, classification)\n",
      "\n",
      "The network adjusts itself to improve accuracy, allowing it to learn and make decisions like a human.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant.Be concise to your answer\"),\n",
    "    (\"human\",\"Explain {concept} in simple terms\")\n",
    "])\n",
    "\n",
    "chain=prompt | llm | StrOutputParser()\n",
    "result=chain.invoke({\"concept\":\"neural networks\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32b29fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a digital ledger that records transactions across a network of computers. It's like a \n",
      "API (Application Programming Interface) is like a messenger between systems. It helps different appl\n",
      "**Recursion**: A process where a function calls itself repeatedly until it reaches a stopping point.\n"
     ]
    }
   ],
   "source": [
    "topics=[\"blockchain\",\"API\",\"recurssion\"]\n",
    "for topic in topics:\n",
    "    result=chain.invoke({\"concept\":topic})\n",
    "    print(result[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f6ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d531c5ef",
   "metadata": {},
   "source": [
    "## 6. Output Parsers - Structured Output\n",
    "\n",
    "Output parsers transform LLM text responses into structured data like **JSON** or **Pydantic models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f1a158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Drishyam\n",
      "Year: 2015\n",
      "Genre: Thriller\n",
      "Reason: It is a well-crafted thriller with a gripping storyline and strong performances.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from typing import List\n",
    "class MovieReccomondation(BaseModel):\n",
    "    \"\"\"A movie recommendation with details\"\"\"\n",
    "    title:str=Field(description=\"The movie title\")\n",
    "    year: int = Field(description=\"Release year\")\n",
    "    genre: str = Field(description=\"Primary genre\")\n",
    "    reason: str = Field(description=\"Why this movie is recommended\")\n",
    "\n",
    "\n",
    "stuructured_llm=llm.with_structured_output(MovieReccomondation)\n",
    "\n",
    "response=stuructured_llm.invoke(\"Recommend any thriller Indian movie from 2010s\")\n",
    "print(f\"Title: {response.title}\")\n",
    "print(f\"Year: {response.year}\")\n",
    "print(f\"Genre: {response.genre}\")\n",
    "print(f\"Reason: {response.reason}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c1087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35ef48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a0f3d8",
   "metadata": {},
   "source": [
    "## 7. Streaming & Batching\n",
    "\n",
    "- **Streaming**: Get responses token-by-token (great for UX)\n",
    "- **Batching**: Process multiple inputs in parallel (great for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec1fb521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " chess\n",
      " player\n",
      " bring\n",
      " a\n",
      " ladder\n",
      " to\n",
      " the\n",
      " tournament\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " wanted\n",
      " to\n",
      " take\n",
      " their\n",
      " game\n",
      " to\n",
      " the\n",
      " next\n",
      " level\n",
      "!\n",
      " (\n",
      "get\n",
      " it\n",
      "?)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Streaming response:\")\n",
    "for chunk in llm.stream(\"Write a joke to a chess player\"):\n",
    "    print(chunk.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d426964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:what is Python?\n",
      "Response:**Python** is a high-level, interpreted programming language that is widely used for various purpose...\n",
      "\n",
      "Prompt:What is Javascript?\n",
      "Response:**JavaScript** is a high-level, dynamic, and interpreted programming language that is primarily used...\n",
      "\n",
      "Prompt:What is Rust?\n",
      "Response:**Rust** is a systems programming language that prioritizes safety, performance, and concurrency. It...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts=[\n",
    "    \"what is Python?\",\n",
    "    \"What is Javascript?\",\n",
    "    \"What is Rust?\",\n",
    "]\n",
    "\n",
    "responses=llm.batch(prompts)\n",
    "# print(responses)\n",
    "for prompt ,response in zip(prompts,responses):\n",
    "    print(f\"Prompt:{prompt}\")\n",
    "    print(f\"Response:{response.content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a242a6",
   "metadata": {},
   "source": [
    "## 8. Tools & Function Calling\n",
    "\n",
    "**Tools** allow the LLM to call external functions. This is the foundation for building **AI agents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "523ab396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city:str)->str:\n",
    "    \"\"\" Get the current weather of the city \"\"\"\n",
    "    weather_data={\n",
    "        \"new delhi\":\"Sunny ,30degree Cel\",\n",
    "        \"mumbai\":\"Sunnay 29 degree Cel\",\n",
    "    }\n",
    "    return weather_data.get(city.lower(),f\"Weather data is not available for {city}\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate(expresssion:str)->str:\n",
    "    \"\"\"Evaluate a mathematical operation\"\"\"\n",
    "    try:\n",
    "        result=eval(expresssion)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        print(\"Exception :{e}\")\n",
    "\n",
    "llm_with_tool=llm.bind_tools([get_weather,calculate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02ca0777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ LLM wants to call tools:\n",
      "  Tool: get_weather\n",
      "  Args: {'city': 'New Delhi'}\n",
      "  Result: Sunny ,30degree Cel\n"
     ]
    }
   ],
   "source": [
    "# Ask the LLM something that requires a tool\n",
    "response = llm_with_tool.invoke(\"What's the weather in New Delhi?\")\n",
    "\n",
    "# Check if the LLM wants to call a tool\n",
    "if response.tool_calls:\n",
    "    print(\"ðŸ”§ LLM wants to call tools:\")\n",
    "    for tool_call in response.tool_calls:\n",
    "        print(f\"  Tool: {tool_call['name']}\")\n",
    "        print(f\"  Args: {tool_call['args']}\")\n",
    "        \n",
    "        # Execute the tool\n",
    "        if tool_call['name'] == 'get_weather':\n",
    "            result = get_weather.invoke(tool_call['args'])\n",
    "            print(f\"  Result: {result}\")\n",
    "else:\n",
    "    print(\"LLM responded directly:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44a91d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ LLM wants to call tools:\n",
      "  Tool: calculate\n",
      "  Args: {'expresssion': '2+3*5 +2'}\n",
      "  Result: Result: 19\n"
     ]
    }
   ],
   "source": [
    "# Ask the LLM something that requires a tool\n",
    "response = llm_with_tool.invoke(\"What is 2+3*5 +2?\")\n",
    "\n",
    "# Check if the LLM wants to call a tool\n",
    "if response.tool_calls:\n",
    "    print(\"ðŸ”§ LLM wants to call tools:\")\n",
    "    for tool_call in response.tool_calls:\n",
    "        print(f\"  Tool: {tool_call['name']}\")\n",
    "        print(f\"  Args: {tool_call['args']}\")\n",
    "        \n",
    "        # Execute the tool\n",
    "        if tool_call['name'] == 'get_weather':\n",
    "            result = get_weather.invoke(tool_call['args'])\n",
    "            print(f\"  Result: {result}\")\n",
    "        if tool_call['name'] == 'calculate':\n",
    "            result = calculate.invoke(tool_call['args'])\n",
    "            print(f\"  Result: {result}\")\n",
    "else:\n",
    "    print(\"LLM responded directly:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8a42492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM responded directly: Newton's third law, also known as the law of reciprocal actions, states that for every action, there is an equal and opposite reaction. This means that when an object exerts a force on another object, the second object will exert an equal and opposite force on the first object. In other words, forces always occur in pairs, and these pairs are equal in magnitude and opposite in direction.\n"
     ]
    }
   ],
   "source": [
    "# Ask the LLM something that requires a tool\n",
    "response = llm_with_tool.invoke(\"What is newton third law?\")\n",
    "\n",
    "# Check if the LLM wants to call a tool\n",
    "if response.tool_calls:\n",
    "    print(\"ðŸ”§ LLM wants to call tools:\")\n",
    "    for tool_call in response.tool_calls:\n",
    "        print(f\"  Tool: {tool_call['name']}\")\n",
    "        print(f\"  Args: {tool_call['args']}\")\n",
    "        \n",
    "        # Execute the tool\n",
    "        if tool_call['name'] == 'get_weather':\n",
    "            result = get_weather.invoke(tool_call['args'])\n",
    "            print(f\"  Result: {result}\")\n",
    "        if tool_call['name'] == 'calculate':\n",
    "            result = calculate.invoke(tool_call['args'])\n",
    "            print(f\"  Result: {result}\")\n",
    "else:\n",
    "    print(\"LLM responded directly:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd93e69",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary\n",
    "\n",
    "In this notebook, you learned the **LangChain Fundamentals**:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **ChatGroq** | Initialize Groq models with temperature, max_tokens |\n",
    "| **Messages** | SystemMessage, HumanMessage, AIMessage for conversations |\n",
    "| **Prompt Templates** | Create reusable prompts with variables |\n",
    "| **LCEL Chains** | Compose components with the `\\|` operator |\n",
    "| **Output Parsers** | Get structured data using Pydantic models |\n",
    "| **Streaming** | Token-by-token response for better UX |\n",
    "| **Batching** | Process multiple inputs efficiently |\n",
    "| **Tools** | Let LLMs call external functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps: RAG Fundamentals\n",
    "In the next notebook, we'll cover **Retrieval Augmented Generation (RAG)**:\n",
    "- Document loaders\n",
    "- Text splitters  \n",
    "- Embeddings\n",
    "- Vector stores\n",
    "- Retrieval chains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "almagenai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
